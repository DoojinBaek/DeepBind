{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from utils import datasets\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "from network import ConvNet_test\n",
    "from Chip import Chip_test, chipseq_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "from utils import dinuc_shuffle, reverse_complement, seq2pad\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_len = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "path = './data/encode/'\n",
    "dataset_names = datasets(path)\n",
    "dataset = dataset_names[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet_seq_logo(nn.Module):\n",
    "    def __init__(self,nummotif,motiflen,poolType,neuType,mode,dropprob, learning_rate,learning_Momentum,sigmaConv,sigmaNeu,beta1,beta2,beta3, device, reverse_complemet_mode):\n",
    "        super(ConvNet_seq_logo, self).__init__()\n",
    "        self.poolType=poolType\n",
    "        self.neuType=neuType\n",
    "        self.mode=mode\n",
    "        self.learning_rate=learning_rate\n",
    "        self.device = device\n",
    "        self.reverse_complemet_mode=reverse_complemet_mode\n",
    "        self.momentum_rate=learning_Momentum\n",
    "        self.sigmaConv=sigmaConv\n",
    "\n",
    "        self.wConv=torch.randn(nummotif,4,motiflen).to(device)\n",
    "        torch.nn.init.normal_(self.wConv,mean=0,std=self.sigmaConv)\n",
    "        self.wConv.requires_grad=True\n",
    "\n",
    "        self.wRect=torch.randn(nummotif).to(device)\n",
    "        torch.nn.init.normal_(self.wRect)\n",
    "        self.wRect=-self.wRect\n",
    "        self.wRect.requires_grad=True\n",
    "\n",
    "        self.dropprob=dropprob\n",
    "        self.sigmaNeu=sigmaNeu\n",
    "        self.wHidden=torch.randn(2*nummotif,32).to(device)\n",
    "        self.wHiddenBias=torch.randn(32).to(device)\n",
    "\n",
    "        if neuType=='nohidden':\n",
    "            if poolType=='maxavg':\n",
    "                self.wNeu=torch.randn(2*nummotif,1).to(device)\n",
    "            else:\n",
    "                self.wNeu=torch.randn(nummotif,1).to(device)\n",
    "            self.wNeuBias=torch.randn(1).to(device)\n",
    "            torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
    "            torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
    "\n",
    "        else:\n",
    "            if poolType=='maxavg':\n",
    "                self.wHidden=torch.randn(2*nummotif,32).to(device)\n",
    "            else:\n",
    "                \n",
    "                self.wHidden=torch.randn(nummotif,32).to(device)\n",
    "            self.wNeu=torch.randn(32,1).to(device)\n",
    "            self.wNeuBias=torch.randn(1).to(device)\n",
    "            self.wHiddenBias=torch.randn(32).to(device)\n",
    "            torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
    "            torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
    "            torch.nn.init.normal_(self.wHidden,mean=0,std=0.3)\n",
    "            torch.nn.init.normal_(self.wHiddenBias,mean=0,std=0.3)\n",
    "            \n",
    "  \n",
    "            self.wHidden.requires_grad=True\n",
    "            self.wHiddenBias.requires_grad=True\n",
    "            #wHiddenBias=tf.Variable(tf.truncated_normal([32,1],mean=0,stddev=sigmaNeu)) #hidden bias for everything\n",
    "\n",
    "        self.wNeu.requires_grad=True\n",
    "        self.wNeuBias.requires_grad=True\n",
    "\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.beta3=beta3\n",
    "    \n",
    "    def divide_two_tensors(self,x):\n",
    "        l=torch.unbind(x)\n",
    "        list1=[l[2*i] for i in range(int(x.shape[0]/2))]\n",
    "        list2=[l[2*i+1] for i in range(int(x.shape[0]/2))]\n",
    "        x1=torch.stack(list1,0)\n",
    "        x2=torch.stack(list2,0)\n",
    "        return x1,x2\n",
    "\n",
    "    def forward_pass(self,x,mask=None,use_mask=False):\n",
    "        conv=F.conv1d(x, self.wConv, bias=self.wRect, stride=1, padding=0)\n",
    "        rect=conv.clamp(min=0)\n",
    "\n",
    "        return rect\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if not  self.reverse_complemet_mode:\n",
    "            out= self.forward_pass(x)\n",
    "        else:\n",
    "            print(\"not supported error\")\n",
    "             \n",
    "            # x1,x2=self.divide_two_tensors(x)\n",
    "            # out1,mask=self.forward_pass(x1)\n",
    "            # out2,_=self.forward_pass(x2,mask,True)\n",
    "            # out=torch.max(out1, out2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Bind Model\n",
    "\n",
    "name = dataset_names[0][2]\n",
    "name = name.split(path)[1].split(\"_AC\")[0]\n",
    "\n",
    "reverse_mode = False\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "checkpoint = torch.load('./Models/'+name+'.pth')\n",
    "\n",
    "best_hyperparameters = torch.load('./Hyperparameters/'+name+'.pth')\n",
    "\n",
    "best_poolType=best_hyperparameters['best_poolType']\n",
    "best_neuType=best_hyperparameters['best_neuType']\n",
    "best_learning_steps=best_hyperparameters['best_learning_steps']\n",
    "best_lr=best_hyperparameters['best_LearningRate']\n",
    "best_droprate=best_hyperparameters['best_dropprob']\n",
    "best_momentum=best_hyperparameters['best_LearningMomentum']\n",
    "best_sigmaConv=best_hyperparameters['best_sigmaConv']\n",
    "best_sigmaNeu=best_hyperparameters['best_sigmaNeu']\n",
    "best_beta1=best_hyperparameters['best_beta1']\n",
    "best_beta2=best_hyperparameters['best_beta2']\n",
    "best_beta3=best_hyperparameters['best_beta3']\n",
    "\n",
    "model = ConvNet_seq_logo(16, 24, best_poolType, best_neuType, 'test', best_lr, best_momentum, best_sigmaConv, best_droprate, best_sigmaNeu, best_beta1, best_beta2, best_beta3, device, reverse_mode).to(device)\n",
    "model.wConv=checkpoint['conv']\n",
    "model.wRect=checkpoint['rect']\n",
    "model.wHidden=checkpoint['wHidden']\n",
    "model.wHiddenBias=checkpoint['wHiddenBias']\n",
    "model.wNeu=checkpoint['wNeu']\n",
    "model.wNeuBias=checkpoint['wNeuBias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = []\n",
    "reverse_complete_mode = False\n",
    "\n",
    "# with gzip.open(dataset, 'rt') as data:\n",
    "#     next(data)\n",
    "#     reader = csv.reader(data, delimiter = '\\t')\n",
    "#     if not reverse_mode:\n",
    "#         for row in reader:\n",
    "#             test_seq.append([row[2], [1]]) # target = 1\n",
    "#             test_seq.append([dinuc_shuffle(row[2]), [0]]) # target = 0\n",
    "#     else:\n",
    "#         for row in reader:\n",
    "#                 test_seq.append([row[2],[1]])\n",
    "#                 test_seq.append([reverse_complement(row[2]),[1]])\n",
    "#                 test_seq.append([dinuc_shuffle(row[2]),[0]])\n",
    "#                 test_seq.append([dinuc_shuffle(reverse_complement(row[2])),[0]])\n",
    "\n",
    "# test_dataset=[]\n",
    "\n",
    "with gzip.open(dataset, 'rt') as data:\n",
    "    next(data)\n",
    "    reader = csv.reader(data,delimiter='\\t')\n",
    "    if not reverse_complete_mode:\n",
    "        for row in reader:\n",
    "                test_seq.append([row[2],[int(row[3])]])\n",
    "    else:\n",
    "        for row in reader:\n",
    "                test_seq.append([row[2],[int(row[3])]])\n",
    "                test_seq.append([reverse_complement(row[2]),[int(row[3])]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "for l in range(len(test_seq)):\n",
    "    test_data.extend([[seq2pad(test_seq[l][0], motiflen=24), test_seq[l][1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=chipseq_dataset(test_data)\n",
    "batchSize=test_dataset.__len__()\n",
    "test_loader = DataLoader(dataset=test_dataset,batch_size=batchSize,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.mode = 'test'\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # which data : 1 ~ 1000\n",
    "k = 0 # motif_detector number\n",
    "output[i][k] # Y_ik \n",
    "\n",
    "zero_seq = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    for k in range(16):\n",
    "        if(np.argmax(output[i][k].cpu().numpy()) <= 0):\n",
    "            zero_seq.extend([[i, k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zero_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_seq = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    for k in range(16):\n",
    "        if([i,k] not in zero_seq):\n",
    "            net_seq.extend([[i, k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15976"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(net_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_position = []\n",
    "\n",
    "for i,k in net_seq:\n",
    "    argmax_position.append(np.argmax(output[i][k].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15976"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(argmax_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84c63b404c7ee130c9845246a39403d500621500816a8f9744527cdc65d245ba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('DeepBind': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
