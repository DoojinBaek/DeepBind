{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n",
      "using gpu :  True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import math \n",
    "import random\n",
    "\n",
    "from os import path\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import bernoulli\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import seq2pad, dinuc_shuffle, complement, reverse_complement, datasets, logsampler, sqrtsampler\n",
    "from Chip import Chip, chipseq_dataset, data_loader, Chip_test\n",
    "from network import ConvNet, ConvNet_test\n",
    "\n",
    "print(torch.__version__)\n",
    "print('using gpu : ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "num_motif = 16\n",
    "motif_len = 24\n",
    "bases = 'ACGT' # DNA bases\n",
    "# basesRNA = 'ACGU' # RNA bases\n",
    "dictReverse = {'A':'T','C':'G','G':'C','T':'A','N':'N'} #dictionary to implement reverse-complement mode\n",
    "reverse_mode=False\n",
    "# Device Configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Hyperparameters\n",
    "num_grid_search = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "lr = 0.001\n",
    "# dataset\n",
    "path = './data/encode/'\n",
    "dataset_names = datasets(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chipseq = Chip(dataset_names[0][0]) # './data/encode/ELK1_GM12878_ELK1_(1277-1)_Stanford_AC.seq.gz'\n",
    "\n",
    "train1, valid1, train2, valid2, train3, valid3, all_data = chipseq.openFile()\n",
    "train_data_loader, valid_data_loader, all_data_loader = data_loader(train1, valid1, train2, valid2, train3, valid3, all_data, batch_size, reverse_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid 1  with Training Fold  1  & learning steps  4000  - AUC :  0.8125727413772527\n",
      "Grid 1  with Training Fold  1  & learning steps  8000  - AUC :  0.825509193618968\n",
      "Grid 1  with Training Fold  1  & learning steps  12000  - AUC :  0.8325873811512909\n",
      "Grid 1  with Training Fold  1  & learning steps  16000  - AUC :  0.8238833034968374\n",
      "Grid 1  with Training Fold  1  & learning steps  20000  - AUC :  0.821144185861479\n",
      "Grid 1  with Training Fold  2  & learning steps  4000  - AUC :  0.832983896248558\n",
      "Grid 1  with Training Fold  2  & learning steps  8000  - AUC :  0.821326387397064\n",
      "Grid 1  with Training Fold  2  & learning steps  12000  - AUC :  0.8256622508652584\n",
      "Grid 1  with Training Fold  2  & learning steps  16000  - AUC :  0.8192746628475952\n",
      "Grid 1  with Training Fold  2  & learning steps  20000  - AUC :  0.8238360902255639\n",
      "Grid 1  with Training Fold  3  & learning steps  4000  - AUC :  0.8273455384492978\n",
      "Grid 1  with Training Fold  3  & learning steps  8000  - AUC :  0.8452762223017862\n",
      "Grid 1  with Training Fold  3  & learning steps  12000  - AUC :  0.8475513466205197\n",
      "Grid 1  with Training Fold  3  & learning steps  16000  - AUC :  0.8499502565938656\n",
      "Grid 1  with Training Fold  3  & learning steps  20000  - AUC :  0.8478279587858535\n",
      "---------------\n",
      "Grid 2  with Training Fold  1  & learning steps  4000  - AUC :  0.8404129848430599\n",
      "Grid 2  with Training Fold  1  & learning steps  8000  - AUC :  0.838691777061702\n",
      "Grid 2  with Training Fold  1  & learning steps  12000  - AUC :  0.8331290925727016\n",
      "Grid 2  with Training Fold  1  & learning steps  16000  - AUC :  0.8342893583164259\n",
      "Grid 2  with Training Fold  1  & learning steps  20000  - AUC :  0.8326717905875801\n",
      "Grid 2  with Training Fold  2  & learning steps  4000  - AUC :  0.841837037037037\n",
      "Grid 2  with Training Fold  2  & learning steps  8000  - AUC :  0.8439527549031309\n",
      "Grid 2  with Training Fold  2  & learning steps  12000  - AUC :  0.8477849862752119\n",
      "Grid 2  with Training Fold  2  & learning steps  16000  - AUC :  0.849900799618093\n",
      "Grid 2  with Training Fold  2  & learning steps  20000  - AUC :  0.8507376536579544\n",
      "Grid 2  with Training Fold  3  & learning steps  4000  - AUC :  0.8211376059195608\n",
      "Grid 2  with Training Fold  3  & learning steps  8000  - AUC :  0.8202230019493177\n",
      "Grid 2  with Training Fold  3  & learning steps  12000  - AUC :  0.8200300911007677\n",
      "Grid 2  with Training Fold  3  & learning steps  16000  - AUC :  0.8196511914707403\n",
      "Grid 2  with Training Fold  3  & learning steps  20000  - AUC :  0.8186785853522696\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Learning\n",
    "\n",
    "AUC_best = 0\n",
    "learning_steps_list = [4000, 8000, 12000, 16000, 20000]\n",
    "\n",
    "for grid in range(num_grid_search):\n",
    "    pool_list = ['max', 'maxavg']\n",
    "    random_pool = random.choice(pool_list)\n",
    "\n",
    "    neuType_list = ['hidden', 'nohidden']\n",
    "    random_neuType = random.choice(neuType_list)\n",
    "\n",
    "    dropout_list = [0.5, 0.75, 1.0]\n",
    "    drop_rate = random.choice(dropout_list)\n",
    "\n",
    "    lr = logsampler(0.0005, 0.05)\n",
    "    momentum_rate = sqrtsampler(0.95, 0.99)\n",
    "    sigmaConv = logsampler(10**-7, 10**-3)\n",
    "    sigmaNeu=logsampler(10**-5,10**-2) \n",
    "    beta1=logsampler(10**-15,10**-3)\n",
    "    beta2=logsampler(10**-10,10**-3)\n",
    "    beta3=logsampler(10**-10,10**-3)\n",
    "\n",
    "    model_AUC = [[], [], []]\n",
    "\n",
    "    for idx in range(3):\n",
    "        model  = ConvNet(16, 24, random_pool, random_neuType, 'training', drop_rate, lr, momentum_rate, sigmaConv, sigmaNeu, beta1, beta2, beta3, device, reverse_complement_mode=reverse_mode).to(device)\n",
    "        if random_neuType == 'nohidden':\n",
    "            optimizer = torch.optim.SGD([model.wConv, model.wRect, model.wNeu, model.wNeuBias], lr = model.lr, momentum=model.momentum_rate, nesterov=True)\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD([model.wConv, model.wRect, model.wNeu, model.wNeuBias, model.wHidden, model.wHiddenBias], lr = model.lr, momentum=model.momentum_rate, nesterov=True)\n",
    "        \n",
    "        train_loader = train_data_loader[idx]\n",
    "        valid_loader = valid_data_loader[idx]\n",
    "\n",
    "        learning_steps = 0\n",
    "        while learning_steps <= 20000:\n",
    "            model.mode = 'training'\n",
    "            auc = []\n",
    "            for i, (data, target) in enumerate(train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                if model.reverse_complement_mode:\n",
    "                    target_2 = torch.randn(int(target.shape[0]/2), 1) # 뭐하는 부분인지 이해 안됨!! -> target.shape[0] = 64\n",
    "                    for i in range(target_2.shape[0]):\n",
    "                        target_2[i] = target[2*i]\n",
    "                    target = target_2.to(device)\n",
    "                \n",
    "                # Forward Pass\n",
    "                output = model(data)\n",
    "                if model.neuType == 'nohidden':\n",
    "                    loss = F.binary_cross_entropy(torch.sigmoid(output), target) + model.beta1*model.wConv.norm() + model.beta3*model.wNeu.norm()\n",
    "                else: \n",
    "                    loss = F.binary_cross_entropy(torch.sigmoid(output), target) + model.beta1*model.wConv.norm() + model.beta2*model.wHidden.norm() + model.beta3*model.wNeu.norm()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                learning_steps+=1\n",
    "\n",
    "                if learning_steps%4000 == 0:\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        model.mode = 'test'\n",
    "                        auc = []\n",
    "                        for i, (data, target) in enumerate(valid_loader):\n",
    "                            data = data.to(device)\n",
    "                            target = target.to(device)\n",
    "                            if model.reverse_complement_mode:\n",
    "                                target_2 = torch.randn(int(target.shape[0]/2), 1)\n",
    "                                for i in range(target_2.shape[0]):\n",
    "                                    target_2[i] = target[2*i]\n",
    "                                target = target_2.to(device)\n",
    "                            # Forward Pass\n",
    "                            output = model(data)\n",
    "                            pred_sig = torch.sigmoid(output)\n",
    "                            pred = pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "\n",
    "                            auc.append(metrics.roc_auc_score(labels, pred))\n",
    "\n",
    "                        model_AUC[idx].append(np.mean(auc))\n",
    "                        print(\"Grid\", grid+1, \" with Training Fold \", idx+1, \" & learning steps \", learning_steps_list[len(model_AUC[idx])-1], \" - AUC : \", np.mean(auc))\n",
    "    \n",
    "    print('---'*5)\n",
    "    for n in range(5):\n",
    "        AUC = (model_AUC[0][n] + model_AUC[1][n] + model_AUC[2][n])/3\n",
    "        if (AUC > AUC_best):\n",
    "            AUC_best = AUC\n",
    "            best_learning_steps = learning_steps_list[n]\n",
    "            best_lr = model.lr\n",
    "            best_momentum = model.momentum_rate\n",
    "            best_neuType = model.neuType\n",
    "            best_poolType = model.poolType\n",
    "            best_sigmaConv = model.sigmaConv\n",
    "            best_droprate = model.droprate\n",
    "            best_sigmaNeu = model.sigmaNeu\n",
    "            best_beta1 = model.beta1\n",
    "            best_beta2 = model.beta2\n",
    "            best_beta3 = model.beta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_poolType= max\n",
      "best_neuType= nohidden\n",
      "best_AUC= 0.835266992879023\n",
      "best_learning_steps= 12000\n",
      "best_LearningRate= 0.013917971521456182\n",
      "best_LearningMomentum= 0.9696122800693299\n",
      "best_sigmaConv= 0.000220810926865881\n",
      "best_dropprob= 0.5\n",
      "best_sigmaNeu= 1.7319444611419063e-05\n",
      "best_beta1= 0.0002115159215273125\n",
      "best_beta2= 1.010343584287067e-09\n",
      "best_beta3= 4.1901621517441283e-05\n"
     ]
    }
   ],
   "source": [
    "# Save Hyperparameters\n",
    "\n",
    "print('best_poolType=',best_poolType)\n",
    "print('best_neuType=',best_neuType)\n",
    "print('best_AUC=',AUC_best)\n",
    "print('best_learning_steps=',best_learning_steps)      \n",
    "print('best_LearningRate=',best_lr)\n",
    "print('best_LearningMomentum=',best_momentum)\n",
    "print('best_sigmaConv=',best_sigmaConv)\n",
    "print('best_dropprob=',best_droprate)\n",
    "print('best_sigmaNeu=',best_sigmaNeu)\n",
    "print('best_beta1=',best_beta1)\n",
    "print('best_beta2=',best_beta2)\n",
    "print('best_beta3=',best_beta3)\n",
    "\n",
    "best_hyperparameters = {'best_poolType': best_poolType,'best_neuType':best_neuType,'best_learning_steps':best_learning_steps,'best_LearningRate':best_lr,\n",
    "                        'best_LearningMomentum':best_momentum,'best_sigmaConv':best_sigmaConv,'best_dropprob':best_droprate,\n",
    "                        'best_sigmaNeu':best_sigmaNeu,'best_beta1':best_beta1, 'best_beta2':best_beta2,'best_beta3':best_beta3}\n",
    "\n",
    "name = dataset_names[0][0]\n",
    "name = name.split(path)[1].split(\"_AC\")[0]\n",
    "torch.save(best_hyperparameters, './Hyperparameters/'+name+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for model  0  =  0.6023595764559327\n",
      "AUC for model  1  =  0.5889436727914461\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "AUC_best = 0\n",
    "learning_steps_list=[4000,8000,12000,16000,20000]\n",
    "\n",
    "best_hyperparameters = torch.load('./Hyperparameters/'+name+'.pth')\n",
    "\n",
    "best_poolType=best_hyperparameters['best_poolType']\n",
    "best_neuType=best_hyperparameters['best_neuType']\n",
    "best_learning_steps=best_hyperparameters['best_learning_steps']\n",
    "best_lr=best_hyperparameters['best_LearningRate']\n",
    "best_droprate=best_hyperparameters['best_dropprob']\n",
    "best_momentum=best_hyperparameters['best_LearningMomentum']\n",
    "best_sigmaConv=best_hyperparameters['best_sigmaConv']\n",
    "best_sigmaNeu=best_hyperparameters['best_sigmaNeu']\n",
    "best_beta1=best_hyperparameters['best_beta1']\n",
    "best_beta2=best_hyperparameters['best_beta2']\n",
    "best_beta3=best_hyperparameters['best_beta3']\n",
    "\n",
    "for number_models in range(6):\n",
    "    model = ConvNet_test(16, 24, best_poolType, best_neuType, 'training', best_lr, best_momentum, best_sigmaConv, best_droprate, best_sigmaNeu, best_beta1, best_beta2, best_beta3, device, False).to(device)\n",
    "\n",
    "    if model.neuType == 'nohidden':\n",
    "        optimizer = torch.optim.SGD([model.wConv, model.wRect, model.wNeu, model.wNeuBias], lr = model.learning_rate, momentum= model.momentum_rate, nesterov=True)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD([model.wConv, model.wRect, model.wNeu, model.wNeuBias, model.wHidden, model.wHiddenBias], lr = model.learning_rate, momentum=model.momentum_rate, nesterov=True)\n",
    "    \n",
    "    train_loader = all_data_loader\n",
    "    valid_loader = all_data_loader\n",
    "    learning_steps = 0\n",
    "\n",
    "    while learning_steps <= best_learning_steps:\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            if reverse_mode:\n",
    "                target_2 = torch.randn(int(target.shape[0]/2), 1)\n",
    "                for i in range(target_2.shape[0]):\n",
    "                    target_2[i] = target[2*i]\n",
    "                target = target_2.to(device)\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            if model.neuType == 'nohidden':\n",
    "                loss = F.binary_cross_entropy(torch.sigmoid(output), target) + model.beta1*model.wConv.norm() + model.beta3*model.wNeu.norm()\n",
    "            else: \n",
    "                loss = F.binary_cross_entropy(torch.sigmoid(output), target) + model.beta1*model.wConv.norm() + model.beta2*model.wHidden.norm() + model.beta3*model.wNeu.norm()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            learning_steps += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.mode = 'test'\n",
    "        auc = []\n",
    "        for i, (data, target) in enumerate(valid_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            if reverse_mode:\n",
    "                target_2 = torch.randn(int(target.shape[0]/2), 1)\n",
    "                for i in range(target_2.shape[0]):\n",
    "                    target_2[i] = target[2*i]\n",
    "                target = target_2.to(device)\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            pred_sig = torch.sigmoid(output)\n",
    "            pred = pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "\n",
    "            auc.append(metrics.roc_auc_score(labels, pred))\n",
    "        \n",
    "        AUC_training = np.mean(auc)\n",
    "        print('AUC for model ', number_models, ' = ', AUC_training)\n",
    "        if AUC_training > AUC_best:\n",
    "            state = {'conv': model.wConv,'rect':model.wRect,'wHidden':model.wHidden,'wHiddenBias':model.wHiddenBias,'wNeu':model.wNeu,'wNeuBias':model.wNeuBias}\n",
    "            # Save Models\n",
    "            torch.save(state, './Models/'+name+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9340005699165481\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('./Models/'+name+'.pth')\n",
    "model = ConvNet_test(16, 24, best_poolType, best_neuType, 'test', best_lr, best_momentum, best_sigmaConv, best_droprate, best_sigmaNeu, best_beta1, best_beta2, best_beta3, device, reverse_mode).to(device)\n",
    "model.wConv=checkpoint['conv']\n",
    "model.wRect=checkpoint['rect']\n",
    "model.wHidden=checkpoint['wHidden']\n",
    "model.wHiddenBias=checkpoint['wHiddenBias']\n",
    "model.wNeu=checkpoint['wNeu']\n",
    "model.wNeuBias=checkpoint['wNeuBias']\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.mode = 'test'\n",
    "    auc = []\n",
    "\n",
    "    for i, (data, target) in enumerate(valid_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        if reverse_mode:\n",
    "            target_2 = torch.randn(int(target.shape[0]/2), 1)\n",
    "            for i in range(target_2.shape[0]):\n",
    "                target_2[i] = target[2*i]\n",
    "            target = target_2.to(device)\n",
    "        \n",
    "        # Forward Pass\n",
    "        output = model(data)\n",
    "        pred_sig = torch.sigmoid(output)\n",
    "        pred = pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
    "        labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "\n",
    "        auc.append(metrics.roc_auc_score(labels, pred))\n",
    "\n",
    "    AUC_training = np.mean(auc)\n",
    "    print(AUC_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on test data =  0.885448\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "chipseq_test=Chip_test(dataset_names[1][0])\n",
    "test_data=chipseq_test.openFile()\n",
    "test_dataset=chipseq_dataset(test_data)\n",
    "batchSize=test_dataset.__len__()\n",
    "test_loader = DataLoader(dataset=test_dataset,batch_size=batchSize,shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "      model.mode='test'\n",
    "      auc=[]\n",
    "     \n",
    "      for i, (data, target) in enumerate(test_loader):\n",
    "          data = data.to(device)\n",
    "          target = target.to(device)\n",
    "          if reverse_mode:\n",
    "              target_2=torch.randn(int(target.shape[0]/2),1)\n",
    "              for i in range(target_2.shape[0]):\n",
    "                target_2[i]=target[2*i]\n",
    "              target=target_2.to(device)\n",
    "          # Forward pass\n",
    "          output = model(data)\n",
    "          pred_sig=torch.sigmoid(output)\n",
    "          pred=pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
    "          labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "          \n",
    "          auc.append(metrics.roc_auc_score(labels, pred))\n",
    "            \n",
    "      AUC_test=np.mean(auc)\n",
    "      print('AUC on test data = ', AUC_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results\n",
    "\n",
    "with open(\"./results/AUC_training.txt\", \"a\") as file:\n",
    "    file.write('TF : ')\n",
    "    file.write(name)\n",
    "    file.write(\" - AUC Train : \")\n",
    "    file.write(str(round(AUC_training, 3)))\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"---\"*20)\n",
    "    file.write(\"\\n\")\n",
    "file.close()\n",
    "\n",
    "with open(\"./results/AUC_testing.txt\", \"a\") as file:\n",
    "    file.write('TF : ')\n",
    "    file.write(name)\n",
    "    file.write(\" - AUC Test : \")\n",
    "    file.write(str(round(AUC_test, 3)))\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"---\"*20)\n",
    "    file.write(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
