{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n",
      "using gpu :  True\n",
      "accelerator :  cu80\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print('using gpu : ', torch.cuda.is_available())\n",
    "accelerator = 'cu80' if torch.cuda.is_available() else 'cpu'\n",
    "print('accelerator : ', accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "codetest = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_motif = 16\n",
    "bases = 'ACGT' # DNA bases\n",
    "# basesRNA = 'ACGU' # RNA bases\n",
    "batch_size = 64\n",
    "dictReverse = {'A':'T','C':'G','G':'C','T':'A','N':'N'} #dictionary to implement reverse-complement mode\n",
    "reverse_mode=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math \n",
    "import random\n",
    "import gzip\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only supporting DNA bases\n",
    "def seq2pad(sequence, motiflen):\n",
    "    rows = len(sequence) + 2*motiflen - 2\n",
    "    S = np.empty([rows, 4])\n",
    "    base = bases\n",
    "    for i in range(rows):\n",
    "        for j in range(4):\n",
    "            if i-motiflen+1<len(sequence) and sequence[i-motiflen+1] == 'N' or i<motiflen-1 or i>len(sequence)+motiflen-2:\n",
    "                S[i,j] = np.float32(0.25)\n",
    "            elif sequence[i-motiflen+1] == base[j]:\n",
    "                S[i,j] = np.float32(1)\n",
    "            else:\n",
    "                S[i,j] = np.float32(0)\n",
    "    return np.transpose(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 0.25 1.   0.   0.   0.   1.   0.   0.   0.   0.25 0.25]\n",
      " [0.25 0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.25 0.25]\n",
      " [0.25 0.25 0.   0.   1.   1.   0.   0.   1.   1.   0.25 0.25]\n",
      " [0.25 0.25 0.   1.   0.   0.   0.   1.   0.   0.   0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "# testing seq2pad\n",
    "\n",
    "if(codetest):\n",
    "    seq_test = 'ATGGATGG'\n",
    "    motiflen_test = 3\n",
    "    s = seq2pad(seq_test, motiflen_test)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dinuc_shuffle(sequence):\n",
    "    b = [sequence[i:i+2] for i in range(0, len(sequence), 2)]\n",
    "    random.shuffle(b)\n",
    "    d = ''.join([str(x) for x in b])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGATGGAT\n"
     ]
    }
   ],
   "source": [
    "# testing dinuc_shuffle\n",
    "\n",
    "if(codetest):\n",
    "    print(dinuc_shuffle(seq_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement(sequence):\n",
    "    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'N': 'N'}\n",
    "    complement_sequence = [complement[base] for base in sequence]\n",
    "    return complement_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'A', 'C', 'C', 'T', 'A', 'C', 'C']\n",
      "['A', 'T', 'G', 'G', 'A', 'T', 'G', 'G']\n"
     ]
    }
   ],
   "source": [
    "# testing complement\n",
    "\n",
    "if(codetest):\n",
    "    print(complement(seq_test))\n",
    "    print(list(seq_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_complement(sequence):\n",
    "    sequence = list(sequence)\n",
    "    sequence.reverse()\n",
    "    return ''.join(complement(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCATCCAT\n"
     ]
    }
   ],
   "source": [
    "# testing reverse_complement\n",
    "\n",
    "if(codetest):\n",
    "    print(reverse_complement(seq_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chip():\n",
    "    def __init__(self, filename, motiflen=24, reverse_complement_mode = reverse_mode):\n",
    "        self.file = filename\n",
    "        self.motiflen = motiflen\n",
    "        self.reverse_complement_mode = reverse_complement_mode\n",
    "    \n",
    "    def openFile(self):\n",
    "        train_dataset = []\n",
    "        with gzip.open(self.file, 'rt') as data:\n",
    "            next(data)\n",
    "            reader = csv.reader(data, delimiter = '\\t')\n",
    "            if not self.reverse_complement_mode:\n",
    "                for row in reader:\n",
    "                    train_dataset.append([seq2pad(row[2], self.motiflen), [1]]) # target = 1\n",
    "                    train_dataset.append([seq2pad(dinuc_shuffle(row[2]), self.motiflen), [0]]) # target = 0\n",
    "            else:\n",
    "                for row in reader:\n",
    "                      train_dataset.append([seq2pad(row[2],self.motiflen),[1]])\n",
    "                      train_dataset.append([seq2pad(reverse_complement(row[2]),self.motiflen),[1]])\n",
    "                      train_dataset.append([seq2pad(dinuc_shuffle(row[2]),self.motiflen),[0]])\n",
    "                      train_dataset.append([seq2pad(dinuc_shuffle(reverse_complement(row[2])),self.motiflen),[0]])\n",
    "        \n",
    "        train_dataset_pad = train_dataset\n",
    "\n",
    "        size = int(len(train_dataset_pad)/3)\n",
    "        firstvalid = train_dataset_pad[:size]\n",
    "        secondvalid = train_dataset_pad[size:2*size]\n",
    "        thirdvalid = train_dataset_pad[2*size:]\n",
    "        firsttrain = secondvalid+thirdvalid\n",
    "        secondtrain = firstvalid+thirdvalid\n",
    "        thirdtrain = firstvalid+secondvalid\n",
    "\n",
    "        return firsttrain, firstvalid, secondtrain, secondvalid, thirdtrain, thirdvalid, train_dataset_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chipseq = Chip('data/encode/ELK1_GM12878_ELK1_(1277-1)_Stanford_AC.seq.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, valid1, train2, valid2, train3, valid3, all_data = chipseq.openFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Chip\n",
    "\n",
    "if(codetest):\n",
    "    dataset = []\n",
    "    with gzip.open('data/encode/ELK1_GM12878_ELK1_(1277-1)_Stanford_AC.seq.gz', 'rt') as data:\n",
    "        next(data)\n",
    "        reader = csv.reader(data, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            print(row[2], \" : \", len(row[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class chipseq_dataset(Dataset):\n",
    "    '''diabetes dataset'''\n",
    "\n",
    "    def __init__(self, xy = None):\n",
    "        self.x_data = np.asarray([element[0] for element in xy],dtype=np.float32)\n",
    "        self.y_data = np.asarray([element[1] for element in xy], dtype=np.float32)\n",
    "        self.x_data = torch.from_numpy(self.x_data)\n",
    "        self.y_data = torch.from_numpy(self.y_data)\n",
    "        self.len = len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_dataset=chipseq_dataset(train1)\n",
    "train2_dataset=chipseq_dataset(train2)\n",
    "train3_dataset=chipseq_dataset(train3)\n",
    "valid1_dataset=chipseq_dataset(valid1)\n",
    "valid2_dataset=chipseq_dataset(valid2)\n",
    "valid3_dataset=chipseq_dataset(valid3)\n",
    "all_data_dataset=chipseq_dataset(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCCGCTCCCTAGCAACGATTGGTTAACGAGGCCTGGTTTCCAGAAATGGGCACTATTTCCGGACGATGTTTGTAGAAGGGAGATCGCTGGGCGGGCGGACT', [1]]\n"
     ]
    }
   ],
   "source": [
    "if(codetest):\n",
    "    train2_test = []\n",
    "    with gzip.open('data/encode/ELK1_GM12878_ELK1_(1277-1)_Stanford_AC.seq.gz', 'rt') as data:\n",
    "        next(data)\n",
    "        reader = csv.reader(data, delimiter = '\\t')\n",
    "        for row in reader:\n",
    "            train2_test.append([row[2], [1]]) # target = 1\n",
    "            train2_test.append([dinuc_shuffle(row[2]), [0]]) # target = 0\n",
    "    \n",
    "    print(train2_test[0]) # -> the first data on 'data/encode/ELK1_GM12878_ELK1_(1277-1)_Stanford_AC.seq.gz'\n",
    "    # to see the data, please refer to the copy of it, 'data/encode/ELK1_GM12878_ELK1_(1277-1)_Stanford_AC.seq copy'\n",
    "\n",
    "# portion of the output of train2_dataset.__gettiem__(0) -> 'CCCG'...\n",
    "    # 0.0000, 0.0000, 0.0000, 0.0000\n",
    "    # 1.0000, 1.0000, 1.0000, 0.0000\n",
    "    # 0.0000, 0.0000, 0.0000, 1.0000\n",
    "    # 0.0000, 0.0000, 0.0000, 0.0000\n",
    "\n",
    "# please note that the output tensor is transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reverse_mode:\n",
    "  train_loader1 = DataLoader(dataset=train1_dataset,batch_size=batch_size,shuffle=False)\n",
    "  train_loader2 = DataLoader(dataset=train2_dataset,batch_size=batch_size,shuffle=False)\n",
    "  train_loader3 = DataLoader(dataset=train3_dataset,batch_size=batch_size,shuffle=False)\n",
    "  valid1_loader = DataLoader(dataset=valid1_dataset,batch_size=batch_size,shuffle=False)\n",
    "  valid2_loader = DataLoader(dataset=valid2_dataset,batch_size=batch_size,shuffle=False)\n",
    "  valid3_loader = DataLoader(dataset=valid3_dataset,batch_size=batch_size,shuffle=False)\n",
    "  alldataset_loader=DataLoader(dataset=all_data_dataset,batch_size=batch_size,shuffle=False)\n",
    "else:\n",
    "  train_loader1 = DataLoader(dataset=train1_dataset,batch_size=batch_size,shuffle=True)\n",
    "  train_loader2 = DataLoader(dataset=train2_dataset,batch_size=batch_size,shuffle=True)\n",
    "  train_loader3 = DataLoader(dataset=train3_dataset,batch_size=batch_size,shuffle=True)\n",
    "  valid1_loader = DataLoader(dataset=valid1_dataset,batch_size=batch_size,shuffle=False)\n",
    "  valid2_loader = DataLoader(dataset=valid2_dataset,batch_size=batch_size,shuffle=False)\n",
    "  valid3_loader = DataLoader(dataset=valid3_dataset,batch_size=batch_size,shuffle=False)\n",
    "  alldataset_loader=DataLoader(dataset=all_data_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "train_dataloader = [train_loader1, train_loader2, train_loader3]\n",
    "valid_dataloader = [valid1_loader, valid2_loader, valid3_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 4\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsampler(a,b):\n",
    "    x = np.random.uniform(low = 0, high = 1)\n",
    "    y = 10**((math.log10(b)-math.log10(a))*x + math.log10(a))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrtsampler(a,b):\n",
    "    x = np.random.uniform(low = 0, hight = 1)\n",
    "    y = (b-a)*math.sqrt(x) + a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_motif, motif_len, poolType, neuType, mode, droprate, lr, momentum_rate, \n",
    "                sigmaConv, sigmaNeu, beta1, beta2, beta3, reverse_complement_mode = reverse_mode):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.poolType = poolType\n",
    "        self.neuType = neuType\n",
    "        self.mode = mode\n",
    "        self.reverse_complement_mode = reverse_complement_mode\n",
    "        self.droprate = droprate\n",
    "        self.lr = lr\n",
    "        self.momentum_rate = momentum_rate\n",
    "        self.sigmaConv = sigmaConv\n",
    "        self.sigmaNeu = sigmaNeu\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta3 = beta3\n",
    "\n",
    "        self.wConv = torch.randn(num_motif, 4, motif_len).to(device)\n",
    "        torch.nn.init.normal_(self.wConv, mean = 0, std = self.sigmaConv)\n",
    "        self.wConv.requires_grad = True\n",
    "\n",
    "        self.wRect = torch.randn(num_motif).to(device)\n",
    "        torch.nn.init.normal_(self.wRect)\n",
    "        self.wRect = -self.wRect\n",
    "        self.wRect.requires_grad = True\n",
    "\n",
    "        if neuType == 'nohidden':\n",
    "            if poolType == 'maxavg':\n",
    "                self.wNeu = torch.randn(2*num_motif, 1).to(device)\n",
    "            else:\n",
    "                self.wNeu = torch.randn(num_motif, 1).to(device)\n",
    "            self.wNeuBias = torch.randn(1).to(device)\n",
    "            torch.nn.init.normal_(self.wNeu, mean=0, std = self.sigmaNeu)\n",
    "            torch.nn.init.normal_(self.wNeuBias, mean=0, std = self.sigmaNeu)\n",
    "        \n",
    "        else:\n",
    "            if poolType == 'maxavg':\n",
    "                self.wHidden = torch.randn(2*num_motif, 32).to(device)\n",
    "            else:\n",
    "                self.wHidden = torch.randn(num_motif, 32).to(device)\n",
    "            self.wHiddenBias = torch.randn(32).to(device)\n",
    "            self.wNeu = torch.randn(32, 1).to(device)\n",
    "            self.wNeuBias = torch.randn(1).to(device)\n",
    "            torch.nn.init.normal_(self.wNeu, mean=0, std = self.sigmaNeu)\n",
    "            torch.nn.init.normal_(self.wNeuBias, mean=0, std = self.sigmaNeu)\n",
    "            torch.nn.init.normal_(self.wHidden, mean=0, std = 0.3)\n",
    "            torch.nn.init.normal_(self.wHiddenBias, mean=0, std = 0.3)\n",
    "\n",
    "            self.wHidden.requires_grad = True\n",
    "            self.wHiddenBias.requires_grad = True\n",
    "        \n",
    "        self.wNeu.requires_grad = True\n",
    "        self.wNeuBias.requires_grad = True\n",
    "    \n",
    "    def divide_two_tensors(self, x):\n",
    "        l = torch.unbind(x)\n",
    "\n",
    "        list1 = [l[2*i] for i in range(int(x.shape[0]/2))]\n",
    "        list2 = [l[2*i + 1] for i in range(int(x.shape[0]/2))]\n",
    "        x1 = torch.stack(list1, 0)\n",
    "        x2 = torch.stack(list2, 0)\n",
    "        return x1, x2\n",
    "    \n",
    "    def forward_pass(self, x, mask=None, use_mask=False):\n",
    "        conv = F.conv1d(x, self.wConv, bias = self.wRect, stride = 1, padding=0)\n",
    "        rect = conv.clamp(min=0) # -> rectification\n",
    "        maxPool, _ = torch.max(rect, dim=2)\n",
    "        if self.poolType == 'maxavg':\n",
    "            avgPool = torch.mean(rect, dim=2)\n",
    "            pool = torch.cat((maxPool, avgPool), 1) # -> pool 순서가 논문과 다르지만 상관 없을것 같다!\n",
    "        else:\n",
    "            pool = maxPool\n",
    "        if(self.neuType == 'nohidden'):\n",
    "            if self.mode == 'training':\n",
    "                if not use_mask:\n",
    "                    mask = bernoulli.rvs(self.droprate, size=len(pool[0]))\n",
    "                    mask = torch.from_numpy(mask).float().to(device)\n",
    "                pooldrop = pool*mask\n",
    "                out = pooldrop @ self.wNeu\n",
    "                out.add_(self.wNeuBias)\n",
    "            else:\n",
    "                out = self.droprate*(pool @ self.wNeu)\n",
    "                out.add_(self.wNeuBias)\n",
    "        else:\n",
    "            hid = pool @ self.wHidden\n",
    "            hid.add_(self.wHiddenBias)\n",
    "            hid = hid.clamp(min = 0) # rectification\n",
    "            if self.mode == 'training':\n",
    "                if not use_mask:\n",
    "                    mask = bernoulli.rvs(self.droprate, size = len(hid[0]))\n",
    "                    mask = torch.from_numpy(mask).float().to(device)\n",
    "                hid_drop = hid*mask\n",
    "                out = self.droprate*(hid@self.wNeu)\n",
    "                out.add_(self.wNeuBias)\n",
    "        \n",
    "        return out, mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.reverse_complement_mode:\n",
    "            out, _ = self.forward_pass(x)\n",
    "        else:\n",
    "            x1, x2 = self.divide_two_tensors(x)\n",
    "            out1, mask = self.forward_pass(x1)\n",
    "            out2, _ = self.forward_pass(x2, mask, True)\n",
    "            out = torch.max(out1, out2)\n",
    "        \n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
