{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n",
      "using gpu :  True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "import math \n",
    "import random\n",
    "import gzip\n",
    "from os import path\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import bernoulli\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import seq2pad, dinuc_shuffle, complement, reverse_complement, datasets, logsampler, sqrtsampler\n",
    "\n",
    "print(torch.__version__)\n",
    "print('using gpu : ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "codetest = True\n",
    "testingChip = False\n",
    "num_motif = 16\n",
    "bases = 'ACGT' # DNA bases\n",
    "# basesRNA = 'ACGU' # RNA bases\n",
    "dictReverse = {'A':'T','C':'G','G':'C','T':'A','N':'N'} #dictionary to implement reverse-complement mode\n",
    "reverse_mode=False\n",
    "# Device Configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Hyperparameters\n",
    "epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "lr = 0.001\n",
    "# dataset\n",
    "path = './data/encode/'\n",
    "dataset_names = datasets(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "testing seq2pad\n",
      "[[0.25 0.25 1.   0.   0.   0.   1.   0.   0.   0.   0.25 0.25]\n",
      " [0.25 0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.25 0.25]\n",
      " [0.25 0.25 0.   0.   1.   1.   0.   0.   1.   1.   0.25 0.25]\n",
      " [0.25 0.25 0.   1.   0.   0.   0.   1.   0.   0.   0.25 0.25]]\n",
      "############################################################\n",
      "\n",
      "############################################################\n",
      "testing dinuc_shuffle\n",
      "GGGGATAT\n",
      "############################################################\n",
      "\n",
      "############################################################\n",
      "testing complement\n",
      "['T', 'A', 'C', 'C', 'T', 'A', 'C', 'C']\n",
      "['A', 'T', 'G', 'G', 'A', 'T', 'G', 'G']\n",
      "############################################################\n",
      "\n",
      "############################################################\n",
      "testing reverse_complement\n",
      "CCATCCAT\n",
      "############################################################\n",
      "\n",
      "############################################################\n",
      "testing datasets\n",
      "./data/encode/ARID3A_K562_ARID3A_(sc-8821)_Stanford_AC.seq.gz\n",
      "./data/encode/ARID3A_K562_ARID3A_(sc-8821)_Stanford_B.seq.gz\n",
      "____________________________________________________________\n",
      "./data/encode/CTCFL_K562_CTCFL_(SC-98982)_HudsonAlpha_AC.seq.gz\n",
      "./data/encode/CTCFL_K562_CTCFL_(SC-98982)_HudsonAlpha_B.seq.gz\n",
      "____________________________________________________________\n",
      "./data/encode/ELK1_GM12878_ELK1_(1277-1)_Stanford_AC.seq.gz\n",
      "./data/encode/ELK1_GM12878_ELK1_(1277-1)_Stanford_B.seq.gz\n",
      "____________________________________________________________\n",
      "./data/encode/FOXA1_HepG2_FOXA1_(SC-101058)_HudsonAlpha_AC.seq.gz\n",
      "./data/encode/FOXA1_HepG2_FOXA1_(SC-101058)_HudsonAlpha_B.seq.gz\n",
      "____________________________________________________________\n",
      "./data/encode/GABPA_GM12878_GABP_HudsonAlpha_AC.seq.gz\n",
      "./data/encode/GABPA_GM12878_GABP_HudsonAlpha_B.seq.gz\n",
      "____________________________________________________________\n",
      "./data/encode/MYC_H1-hESC_c-Myc_Stanford_AC.seq.gz\n",
      "./data/encode/MYC_H1-hESC_c-Myc_Stanford_B.seq.gz\n",
      "____________________________________________________________\n",
      "./data/encode/REST_GM12878_NRSF_HudsonAlpha_AC.seq.gz\n",
      "./data/encode/REST_GM12878_NRSF_HudsonAlpha_B.seq.gz\n",
      "____________________________________________________________\n",
      "./data/encode/SP1_GM12878_SP1_HudsonAlpha_AC.seq.gz\n",
      "./data/encode/SP1_GM12878_SP1_HudsonAlpha_B.seq.gz\n",
      "____________________________________________________________\n",
      "./data/encode/USF1_HepG2_USF-1_HudsonAlpha_AC.seq.gz\n",
      "./data/encode/USF1_HepG2_USF-1_HudsonAlpha_B.seq.gz\n",
      "____________________________________________________________\n",
      "./data/encode/ZBTB7A_HepG2_ZBTB7A_(SC-34508)_HudsonAlpha_AC.seq.gz\n",
      "./data/encode/ZBTB7A_HepG2_ZBTB7A_(SC-34508)_HudsonAlpha_B.seq.gz\n",
      "____________________________________________________________\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "# testing seq2pad\n",
    "if(codetest):\n",
    "    print(\"\\n\"+\"###\"*20)\n",
    "    print(\"testing seq2pad\")\n",
    "    seq_test = 'ATGGATGG'\n",
    "    motiflen_test = 3\n",
    "    s = seq2pad(seq_test, motiflen_test)\n",
    "    print(s)\n",
    "    print(\"###\"*20)\n",
    "\n",
    "# testing dinuc_shuffle\n",
    "if(codetest):\n",
    "    print(\"\\n\"+\"###\"*20)\n",
    "    print(\"testing dinuc_shuffle\")\n",
    "    print(dinuc_shuffle(seq_test))\n",
    "    print(\"###\"*20)\n",
    "\n",
    "# testing complement\n",
    "if(codetest):\n",
    "    print(\"\\n\"+\"###\"*20)\n",
    "    print(\"testing complement\")\n",
    "    print(complement(seq_test))\n",
    "    print(list(seq_test))\n",
    "    print(\"###\"*20)\n",
    "\n",
    "# testing reverse_complement\n",
    "if(codetest):\n",
    "    print(\"\\n\"+\"###\"*20)\n",
    "    print(\"testing reverse_complement\")\n",
    "    print(reverse_complement(seq_test))\n",
    "    print(\"###\"*20)\n",
    "\n",
    "# testing datasets\n",
    "if(codetest):\n",
    "    print(\"\\n\"+\"###\"*20)\n",
    "    print(\"testing datasets\")\n",
    "    for i in range(len(dataset_names[0])):\n",
    "        print(dataset_names[0][i])\n",
    "        print(dataset_names[1][i])\n",
    "        print(\"___\"*20)\n",
    "    print(\"###\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chip():\n",
    "    def __init__(self, filename, motiflen=24, reverse_complement_mode = reverse_mode):\n",
    "        self.file = filename\n",
    "        self.motiflen = motiflen\n",
    "        self.reverse_complement_mode = reverse_complement_mode\n",
    "    \n",
    "    def openFile(self):\n",
    "        train_dataset = []\n",
    "        with gzip.open(self.file, 'rt') as data:\n",
    "            next(data)\n",
    "            reader = csv.reader(data, delimiter = '\\t')\n",
    "            if not self.reverse_complement_mode:\n",
    "                for row in reader:\n",
    "                    train_dataset.append([seq2pad(row[2], self.motiflen), [1]]) # target = 1\n",
    "                    train_dataset.append([seq2pad(dinuc_shuffle(row[2]), self.motiflen), [0]]) # target = 0\n",
    "            else:\n",
    "                for row in reader:\n",
    "                      train_dataset.append([seq2pad(row[2],self.motiflen),[1]])\n",
    "                      train_dataset.append([seq2pad(reverse_complement(row[2]),self.motiflen),[1]])\n",
    "                      train_dataset.append([seq2pad(dinuc_shuffle(row[2]),self.motiflen),[0]])\n",
    "                      train_dataset.append([seq2pad(dinuc_shuffle(reverse_complement(row[2])),self.motiflen),[0]])\n",
    "        \n",
    "        train_dataset_pad = train_dataset\n",
    "\n",
    "        size = int(len(train_dataset_pad)/3)\n",
    "        firstvalid = train_dataset_pad[:size]\n",
    "        secondvalid = train_dataset_pad[size:2*size]\n",
    "        thirdvalid = train_dataset_pad[2*size:]\n",
    "        firsttrain = secondvalid+thirdvalid\n",
    "        secondtrain = firstvalid+thirdvalid\n",
    "        thirdtrain = firstvalid+secondvalid\n",
    "\n",
    "        return firsttrain, firstvalid, secondtrain, secondvalid, thirdtrain, thirdvalid, train_dataset_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "chipseq = Chip(dataset_names[0][2]) #'./data/encode/ELK1_GM12878_ELK1_(1277-1)_Stanford_AC.seq.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, valid1, train2, valid2, train3, valid3, all_data = chipseq.openFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Chip\n",
    "if(codetest):\n",
    "    if(testingChip):\n",
    "        dataset = []\n",
    "        with gzip.open(dataset_names[0][2], 'rt') as data:\n",
    "            next(data)\n",
    "            reader = csv.reader(data, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                print(row[2], \" : \", len(row[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chipseq_dataset(Dataset):\n",
    "    def __init__(self, xy = None):\n",
    "        self.x_data = np.asarray([element[0] for element in xy], dtype=np.float32)\n",
    "        self.y_data = np.asarray([element[1] for element in xy], dtype=np.float32)\n",
    "        self.x_data = torch.from_numpy(self.x_data)\n",
    "        self.y_data = torch.from_numpy(self.y_data)\n",
    "        self.len = len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_dataset=chipseq_dataset(train1)\n",
    "train2_dataset=chipseq_dataset(train2)\n",
    "train3_dataset=chipseq_dataset(train3)\n",
    "valid1_dataset=chipseq_dataset(valid1)\n",
    "valid2_dataset=chipseq_dataset(valid2)\n",
    "valid3_dataset=chipseq_dataset(valid3)\n",
    "all_data_dataset=chipseq_dataset(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCCGCTCCCTAGCAACGATTGGTTAACGAGGCCTGGTTTCCAGAAATGGGCACTATTTCCGGACGATGTTTGTAGAAGGGAGATCGCTGGGCGGGCGGACT', [1]]\n"
     ]
    }
   ],
   "source": [
    "if(codetest):\n",
    "    train2_test = []\n",
    "    with gzip.open(dataset_names[0][2], 'rt') as data:\n",
    "        next(data)\n",
    "        reader = csv.reader(data, delimiter = '\\t')\n",
    "        for row in reader:\n",
    "            train2_test.append([row[2], [1]]) # target = 1\n",
    "            train2_test.append([dinuc_shuffle(row[2]), [0]]) # target = 0\n",
    "    \n",
    "    print(train2_test[0]) # -> the first data on 'data/encode/ELK1_GM12878_ELK1_(1277-1)_Stanford_AC.seq.gz'\n",
    "    # to see the data, please refer to the copy of it, 'data/encode/ELK1_GM12878_ELK1_(1277-1)_Stanford_AC.seq copy'\n",
    "\n",
    "# portion of the output of train2_dataset.__gettiem__(0) -> 'CCCG'...\n",
    "    # 0.0000, 0.0000, 0.0000, 0.0000\n",
    "    # 1.0000, 1.0000, 1.0000, 0.0000\n",
    "    # 0.0000, 0.0000, 0.0000, 1.0000\n",
    "    # 0.0000, 0.0000, 0.0000, 0.0000\n",
    "\n",
    "# please note that the output tensor is transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reverse_mode:\n",
    "  train_loader1 = DataLoader(dataset=train1_dataset,batch_size=batch_size,shuffle=False)\n",
    "  train_loader2 = DataLoader(dataset=train2_dataset,batch_size=batch_size,shuffle=False)\n",
    "  train_loader3 = DataLoader(dataset=train3_dataset,batch_size=batch_size,shuffle=False)\n",
    "  valid1_loader = DataLoader(dataset=valid1_dataset,batch_size=batch_size,shuffle=False)\n",
    "  valid2_loader = DataLoader(dataset=valid2_dataset,batch_size=batch_size,shuffle=False)\n",
    "  valid3_loader = DataLoader(dataset=valid3_dataset,batch_size=batch_size,shuffle=False)\n",
    "  alldataset_loader=DataLoader(dataset=all_data_dataset,batch_size=batch_size,shuffle=False)\n",
    "else:\n",
    "  train_loader1 = DataLoader(dataset=train1_dataset,batch_size=batch_size,shuffle=True)\n",
    "  train_loader2 = DataLoader(dataset=train2_dataset,batch_size=batch_size,shuffle=True)\n",
    "  train_loader3 = DataLoader(dataset=train3_dataset,batch_size=batch_size,shuffle=True)\n",
    "  valid1_loader = DataLoader(dataset=valid1_dataset,batch_size=batch_size,shuffle=False)\n",
    "  valid2_loader = DataLoader(dataset=valid2_dataset,batch_size=batch_size,shuffle=False)\n",
    "  valid3_loader = DataLoader(dataset=valid3_dataset,batch_size=batch_size,shuffle=False)\n",
    "  alldataset_loader=DataLoader(dataset=all_data_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "train_dataloader = [train_loader1, train_loader2, train_loader3]\n",
    "valid_dataloader = [valid1_loader, valid2_loader, valid3_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_motif, motif_len, poolType, neuType, mode, droprate, lr, momentum_rate, \n",
    "                sigmaConv, sigmaNeu, beta1, beta2, beta3, reverse_complement_mode = reverse_mode):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.poolType = poolType\n",
    "        self.neuType = neuType\n",
    "        self.mode = mode\n",
    "        self.reverse_complement_mode = reverse_complement_mode\n",
    "        self.droprate = droprate\n",
    "        self.lr = lr\n",
    "        self.momentum_rate = momentum_rate\n",
    "        self.sigmaConv = sigmaConv\n",
    "        self.sigmaNeu = sigmaNeu\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta3 = beta3\n",
    "\n",
    "        self.wConv = torch.randn(num_motif, 4, motif_len).to(device)\n",
    "        torch.nn.init.normal_(self.wConv, mean = 0, std = self.sigmaConv)\n",
    "        self.wConv.requires_grad = True\n",
    "\n",
    "        self.wRect = torch.randn(num_motif).to(device)\n",
    "        torch.nn.init.normal_(self.wRect)\n",
    "        self.wRect = -self.wRect\n",
    "        self.wRect.requires_grad = True\n",
    "\n",
    "        if neuType == 'nohidden':\n",
    "            if poolType == 'maxavg':\n",
    "                self.wNeu = torch.randn(2*num_motif, 1).to(device)\n",
    "            else:\n",
    "                self.wNeu = torch.randn(num_motif, 1).to(device)\n",
    "            self.wNeuBias = torch.randn(1).to(device)\n",
    "            torch.nn.init.normal_(self.wNeu, mean=0, std = self.sigmaNeu)\n",
    "            torch.nn.init.normal_(self.wNeuBias, mean=0, std = self.sigmaNeu)\n",
    "        \n",
    "        else:\n",
    "            if poolType == 'maxavg':\n",
    "                self.wHidden = torch.randn(2*num_motif, 32).to(device)\n",
    "            else:\n",
    "                self.wHidden = torch.randn(num_motif, 32).to(device)\n",
    "            self.wHiddenBias = torch.randn(32).to(device)\n",
    "            self.wNeu = torch.randn(32, 1).to(device)\n",
    "            self.wNeuBias = torch.randn(1).to(device)\n",
    "            torch.nn.init.normal_(self.wNeu, mean=0, std = self.sigmaNeu)\n",
    "            torch.nn.init.normal_(self.wNeuBias, mean=0, std = self.sigmaNeu)\n",
    "            torch.nn.init.normal_(self.wHidden, mean=0, std = 0.3)\n",
    "            torch.nn.init.normal_(self.wHiddenBias, mean=0, std = 0.3)\n",
    "\n",
    "            self.wHidden.requires_grad = True\n",
    "            self.wHiddenBias.requires_grad = True\n",
    "        \n",
    "        self.wNeu.requires_grad = True\n",
    "        self.wNeuBias.requires_grad = True\n",
    "    \n",
    "    def divide_two_tensors(self, x):\n",
    "        l = torch.unbind(x)\n",
    "\n",
    "        list1 = [l[2*i] for i in range(int(x.shape[0]/2))]\n",
    "        list2 = [l[2*i + 1] for i in range(int(x.shape[0]/2))]\n",
    "        x1 = torch.stack(list1, 0)\n",
    "        x2 = torch.stack(list2, 0)\n",
    "        return x1, x2\n",
    "    \n",
    "    def forward_pass(self, x, mask=None, use_mask=False):\n",
    "        conv = F.conv1d(x, self.wConv, bias = self.wRect, stride = 1, padding=0)\n",
    "        rect = conv.clamp(min=0) # -> rectification\n",
    "        # global out # ???? \n",
    "        maxPool, _ = torch.max(rect, dim=2)\n",
    "        if self.poolType == 'maxavg':\n",
    "            avgPool = torch.mean(rect, dim=2)\n",
    "            pool = torch.cat((maxPool, avgPool), 1) # -> pool 순서가 논문과 다르지만 상관 없을것 같다!\n",
    "        else:\n",
    "            pool = maxPool\n",
    "        if(self.neuType == 'nohidden'):\n",
    "            if self.mode == 'training':\n",
    "                if not use_mask:\n",
    "                    mask = bernoulli.rvs(self.droprate, size=len(pool[0]))\n",
    "                    mask = torch.from_numpy(mask).float().to(device)\n",
    "                pooldrop = pool*mask\n",
    "                out = pooldrop @ self.wNeu\n",
    "                out.add_(self.wNeuBias)\n",
    "            else:\n",
    "                out = self.droprate*(pool @ self.wNeu)\n",
    "                out.add_(self.wNeuBias)\n",
    "        else:\n",
    "            hid = pool @ self.wHidden\n",
    "            hid.add_(self.wHiddenBias)\n",
    "            hid = hid.clamp(min = 0) # rectification\n",
    "            if self.mode == 'training':\n",
    "                if not use_mask:\n",
    "                    mask = bernoulli.rvs(self.droprate, size = len(hid[0]))\n",
    "                    mask = torch.from_numpy(mask).float().to(device)\n",
    "                hid_drop = hid*mask\n",
    "                out = self.droprate*(hid@self.wNeu)\n",
    "                out.add_(self.wNeuBias)\n",
    "            else:\n",
    "                out = self.droprate*(hid@self.wNeu)\n",
    "                out.add_(self.wNeuBias)\n",
    "        \n",
    "        return out, mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.reverse_complement_mode:\n",
    "            out, _ = self.forward_pass(x)\n",
    "        else:\n",
    "            x1, x2 = self.divide_two_tensors(x)\n",
    "            out1, mask = self.forward_pass(x1)\n",
    "            out2, _ = self.forward_pass(x2, mask, True)\n",
    "            out = torch.max(out1, out2)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC performance of traing fold- 1  with learning steps - 4000  :  0.8977259655377301\n",
      "AUC performance of traing fold- 1  with learning steps - 8000  :  0.891599881164587\n",
      "AUC performance of traing fold- 1  with learning steps - 12000  :  0.8900002376708258\n",
      "AUC performance of traing fold- 1  with learning steps - 16000  :  0.8899142008318479\n",
      "AUC performance of traing fold- 1  with learning steps - 20000  :  0.8855037433155081\n",
      "AUC performance of traing fold- 2  with learning steps - 4000  :  0.8104884135472372\n",
      "AUC performance of traing fold- 2  with learning steps - 8000  :  0.7990326797385621\n",
      "AUC performance of traing fold- 2  with learning steps - 12000  :  0.8038925133689839\n",
      "AUC performance of traing fold- 2  with learning steps - 16000  :  0.8017612596553773\n",
      "AUC performance of traing fold- 2  with learning steps - 20000  :  0.8010915626856803\n",
      "AUC performance of traing fold- 3  with learning steps - 4000  :  0.7733527959331881\n",
      "AUC performance of traing fold- 3  with learning steps - 8000  :  0.7703705156136529\n",
      "AUC performance of traing fold- 3  with learning steps - 12000  :  0.7625161946259984\n",
      "AUC performance of traing fold- 3  with learning steps - 16000  :  0.765451343500363\n",
      "AUC performance of traing fold- 3  with learning steps - 20000  :  0.7612744371822804\n",
      "---------------\n",
      "AUC performance of traing fold- 1  with learning steps - 4000  :  0.7240919191919192\n",
      "AUC performance of traing fold- 1  with learning steps - 8000  :  0.8759358288770054\n",
      "AUC performance of traing fold- 1  with learning steps - 12000  :  0.875731788472965\n",
      "AUC performance of traing fold- 1  with learning steps - 16000  :  0.876374450386215\n",
      "AUC performance of traing fold- 1  with learning steps - 20000  :  0.8756581105169341\n",
      "AUC performance of traing fold- 2  with learning steps - 4000  :  0.7806530005941771\n",
      "AUC performance of traing fold- 2  with learning steps - 8000  :  0.8036136660724896\n",
      "AUC performance of traing fold- 2  with learning steps - 12000  :  0.80079275103981\n",
      "AUC performance of traing fold- 2  with learning steps - 16000  :  0.7995176470588234\n",
      "AUC performance of traing fold- 2  with learning steps - 20000  :  0.7957704099821746\n",
      "AUC performance of traing fold- 3  with learning steps - 4000  :  0.6440809005083514\n",
      "AUC performance of traing fold- 3  with learning steps - 8000  :  0.7657541031227305\n",
      "AUC performance of traing fold- 3  with learning steps - 12000  :  0.7707541031227306\n",
      "AUC performance of traing fold- 3  with learning steps - 16000  :  0.7711236020334059\n",
      "AUC performance of traing fold- 3  with learning steps - 20000  :  0.7717140159767611\n",
      "---------------\n",
      "AUC performance of traing fold- 1  with learning steps - 4000  :  0.882749019607843\n",
      "AUC performance of traing fold- 1  with learning steps - 8000  :  0.8705566250742721\n",
      "AUC performance of traing fold- 1  with learning steps - 12000  :  0.8706686868686869\n",
      "AUC performance of traing fold- 1  with learning steps - 16000  :  0.8717854426619133\n",
      "AUC performance of traing fold- 1  with learning steps - 20000  :  0.8572180629827688\n",
      "AUC performance of traing fold- 2  with learning steps - 4000  :  0.8178512180629828\n",
      "AUC performance of traing fold- 2  with learning steps - 8000  :  0.8203035056446821\n",
      "AUC performance of traing fold- 2  with learning steps - 12000  :  0.8149618538324421\n",
      "AUC performance of traing fold- 2  with learning steps - 16000  :  0.8136971479500892\n",
      "AUC performance of traing fold- 2  with learning steps - 20000  :  0.8114169934640524\n",
      "AUC performance of traing fold- 3  with learning steps - 4000  :  0.7830368917937546\n",
      "AUC performance of traing fold- 3  with learning steps - 8000  :  0.773747494553377\n",
      "AUC performance of traing fold- 3  with learning steps - 12000  :  0.7697301379811183\n",
      "AUC performance of traing fold- 3  with learning steps - 16000  :  0.7592836601307189\n",
      "AUC performance of traing fold- 3  with learning steps - 20000  :  0.742732389251997\n",
      "---------------\n",
      "AUC performance of traing fold- 1  with learning steps - 4000  :  0.8518087938205585\n",
      "AUC performance of traing fold- 1  with learning steps - 8000  :  0.8420976827094475\n",
      "AUC performance of traing fold- 1  with learning steps - 12000  :  0.8379975638740345\n",
      "AUC performance of traing fold- 1  with learning steps - 16000  :  0.8331954842543077\n",
      "AUC performance of traing fold- 1  with learning steps - 20000  :  0.8271926916221034\n",
      "AUC performance of traing fold- 2  with learning steps - 4000  :  0.810875935828877\n",
      "AUC performance of traing fold- 2  with learning steps - 8000  :  0.8093176470588235\n",
      "AUC performance of traing fold- 2  with learning steps - 12000  :  0.8063238265002971\n",
      "AUC performance of traing fold- 2  with learning steps - 16000  :  0.8057502079619727\n",
      "AUC performance of traing fold- 2  with learning steps - 20000  :  0.8006462269756387\n",
      "AUC performance of traing fold- 3  with learning steps - 4000  :  0.7762582425562818\n",
      "AUC performance of traing fold- 3  with learning steps - 8000  :  0.7681989832970224\n",
      "AUC performance of traing fold- 3  with learning steps - 12000  :  0.7622305010893246\n",
      "AUC performance of traing fold- 3  with learning steps - 16000  :  0.7595562091503267\n",
      "AUC performance of traing fold- 3  with learning steps - 20000  :  0.7564799564270154\n",
      "---------------\n",
      "AUC performance of traing fold- 1  with learning steps - 4000  :  0.864574688057041\n",
      "AUC performance of traing fold- 1  with learning steps - 8000  :  0.8676897207367796\n",
      "AUC performance of traing fold- 1  with learning steps - 12000  :  0.8702237670825906\n",
      "AUC performance of traing fold- 1  with learning steps - 16000  :  0.8661656565656566\n",
      "AUC performance of traing fold- 1  with learning steps - 20000  :  0.8649812240047534\n",
      "AUC performance of traing fold- 2  with learning steps - 4000  :  0.8086670231729055\n",
      "AUC performance of traing fold- 2  with learning steps - 8000  :  0.8047270350564469\n",
      "AUC performance of traing fold- 2  with learning steps - 12000  :  0.8033506833036245\n",
      "AUC performance of traing fold- 2  with learning steps - 16000  :  0.8008645276292335\n",
      "AUC performance of traing fold- 2  with learning steps - 20000  :  0.8019100415923945\n",
      "AUC performance of traing fold- 3  with learning steps - 4000  :  0.7718973129992739\n",
      "AUC performance of traing fold- 3  with learning steps - 8000  :  0.7646560639070443\n",
      "AUC performance of traing fold- 3  with learning steps - 12000  :  0.7584429920116195\n",
      "AUC performance of traing fold- 3  with learning steps - 16000  :  0.7547170660856936\n",
      "AUC performance of traing fold- 3  with learning steps - 20000  :  0.752081045751634\n",
      "---------------\n",
      "best_poolType= maxavg\n",
      "best_neuType= hidden\n",
      "best_AUC= 0.82787904315486\n",
      "best_learning_steps= 4000\n",
      "best_LearningRate= 0.027510822278323632\n",
      "best_LearningMomentum= 0.9703269382387888\n",
      "best_sigmaConv= 3.668387566553053e-05\n",
      "best_dropprob= 0.5\n",
      "best_sigmaNeu= 0.00020849106665675728\n",
      "best_beta1= 3.144639044013777e-06\n",
      "best_beta2= 1.2803764178818175e-07\n",
      "best_beta3= 6.831126377991214e-06\n"
     ]
    }
   ],
   "source": [
    "AUC_best = 0\n",
    "learning_steps_list = [4000, 8000, 12000, 16000, 20000]\n",
    "\n",
    "for number in range(5):\n",
    "    pool_list = ['max', 'maxavg']\n",
    "    random_pool = random.choice(pool_list)\n",
    "\n",
    "    neuType_list = ['hidden', 'nohidden']\n",
    "    random_neuType = random.choice(neuType_list)\n",
    "\n",
    "    dropout_list = [0.5, 0.75, 1.0]\n",
    "    drop_rate = random.choice(dropout_list)\n",
    "\n",
    "    lr = logsampler(0.0005, 0.05)\n",
    "    momentum_rate = sqrtsampler(0.95, 0.99)\n",
    "    sigmaConv = logsampler(10**-7, 10**-3)\n",
    "    sigmaNeu=logsampler(10**-5,10**-2) \n",
    "    beta1=logsampler(10**-15,10**-3)\n",
    "    beta2=logsampler(10**-10,10**-3)\n",
    "    beta3=logsampler(10**-10,10**-3)\n",
    "\n",
    "    model_AUC = [[], [], []]\n",
    "\n",
    "    for kk in range(3):\n",
    "        model  = ConvNet(16, 24, random_pool, random_neuType, 'training', drop_rate, lr, momentum_rate, sigmaConv, sigmaNeu, beta1, beta2, beta3, reverse_complement_mode=reverse_mode).to(device)\n",
    "        if random_neuType == 'nohidden':\n",
    "            optimizer = torch.optim.SGD([model.wConv, model.wRect, model.wNeu, model.wNeuBias], lr = model.lr, momentum=model.momentum_rate, nesterov=True)\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD([model.wConv, model.wRect, model.wNeu, model.wNeuBias, model.wHidden, model.wHiddenBias], lr = model.lr, momentum=model.momentum_rate, nesterov=True)\n",
    "        \n",
    "        train_loader = train_dataloader[kk]\n",
    "        valid_loader = valid_dataloader[kk]\n",
    "\n",
    "        learning_steps = 0\n",
    "        while learning_steps <= 20000:\n",
    "            model.mode = 'training'\n",
    "            auc = []\n",
    "            for i, (data, target) in enumerate(train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                if model.reverse_complement_mode:\n",
    "                    target_2 = torch.randn(int(target.shape[0]/2), 1) # 뭐하는 부분인지 이해 안됨!! -> target.shape[0] = 64\n",
    "                    for i in range(target_2.shape[0]):\n",
    "                        target_2[i] = target[2*i]\n",
    "                    target = target_2.to(device)\n",
    "                \n",
    "                # Forward Pass\n",
    "                output = model(data)\n",
    "                if model.neuType == 'nohidden':\n",
    "                    loss = F.binary_cross_entropy(torch.sigmoid(output), target) + model.beta1*model.wConv.norm() + model.beta3*model.wNeu.norm()\n",
    "                else: \n",
    "                    loss = F.binary_cross_entropy(torch.sigmoid(output), target) + model.beta1*model.wConv.norm() + model.beta2*model.wHidden.norm() + model.beta3*model.wNeu.norm()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                learning_steps+=1\n",
    "\n",
    "                if learning_steps%4000 == 0:\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        model.mode = 'test'\n",
    "                        auc = []\n",
    "                        for i, (data, target) in enumerate(valid_loader):\n",
    "                            data = data.to(device)\n",
    "                            target = target.to(device)\n",
    "                            if model.reverse_complement_mode:\n",
    "                                target_2 = torch.randn(int(target.shape[0]/2), 1)\n",
    "                                for i in range(target_2.shape[0]):\n",
    "                                    target_2[i] = target[2*i]\n",
    "                                target = target_2.to(device)\n",
    "                            # Forward Pass\n",
    "                            output = model(data)\n",
    "                            pred_sig = torch.sigmoid(output)\n",
    "                            pred = pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "\n",
    "                            auc.append(metrics.roc_auc_score(labels, pred))\n",
    "\n",
    "                        model_AUC[kk].append(np.mean(auc))\n",
    "                        print('AUC performance of traing fold-', kk+1, ' with learning steps -', learning_steps_list[len(model_AUC[kk])-1], \" : \", np.mean(auc))\n",
    "    \n",
    "    print('---'*5)\n",
    "    for n in range(5):\n",
    "        AUC = (model_AUC[0][n] + model_AUC[1][n] + model_AUC[2][n])/3\n",
    "        if (AUC > AUC_best):\n",
    "            AUC_best = AUC\n",
    "            best_learning_steps = learning_steps_list[n]\n",
    "            best_lr = model.lr\n",
    "            best_momentum = model.momentum_rate\n",
    "            best_neuType = model.neuType\n",
    "            best_poolType = model.poolType\n",
    "            best_sigmaConv = model.sigmaConv\n",
    "            best_droprate = model.droprate\n",
    "            best_sigmaNeu = model.sigmaNeu\n",
    "            best_beta1 = model.beta1\n",
    "            best_beta2 = model.beta2\n",
    "            best_beta3 = model.beta3\n",
    "\n",
    "print('best_poolType=',best_poolType)\n",
    "print('best_neuType=',best_neuType)\n",
    "print('best_AUC=',AUC_best)\n",
    "print('best_learning_steps=',best_learning_steps)      \n",
    "print('best_LearningRate=',best_lr)\n",
    "print('best_LearningMomentum=',best_momentum)\n",
    "print('best_sigmaConv=',best_sigmaConv)\n",
    "print('best_dropprob=',best_droprate)\n",
    "print('best_sigmaNeu=',best_sigmaNeu)\n",
    "print('best_beta1=',best_beta1)\n",
    "print('best_beta2=',best_beta2)\n",
    "print('best_beta3=',best_beta3)\n",
    "\n",
    "best_hyperparameters = {'best_poolType': best_poolType,'best_neuType':best_neuType,'best_learning_steps':best_learning_steps,'best_LearningRate':best_lr,\n",
    "                        'best_LearningMomentum':best_momentum,'best_sigmaConv':best_sigmaConv,'best_dropprob':best_droprate,\n",
    "                        'best_sigmaNeu':best_sigmaNeu,'best_beta1':best_beta1, 'best_beta2':best_beta2,'best_beta3':best_beta3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Hyperparameters\n",
    "\n",
    "name = dataset_names[0][2]\n",
    "name = name.split(path)[1].split(\"_AC\")[0]\n",
    "torch.save(best_hyperparameters, './Hyperparameters/'+name+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet_test(nn.Module):\n",
    "    def __init__(self,nummotif,motiflen,poolType,neuType,mode,dropprob, learning_rate,learning_Momentum,sigmaConv,sigmaNeu,beta1,beta2,beta3,learning_steps, reverse_complemet_mode):\n",
    "        super(ConvNet_test, self).__init__()\n",
    "        self.poolType=poolType\n",
    "        self.neuType=neuType\n",
    "        self.mode=mode\n",
    "        self.learning_rate=learning_rate\n",
    "        self.reverse_complemet_mode=reverse_complemet_mode\n",
    "        self.momentum_rate=learning_Momentum\n",
    "        self.sigmaConv=sigmaConv\n",
    "        self.wConv=torch.randn(nummotif,4,motiflen).to(device)\n",
    "        torch.nn.init.normal_(self.wConv,mean=0,std=self.sigmaConv)\n",
    "        self.wConv.requires_grad=True\n",
    "        self.wRect=torch.randn(nummotif).to(device)\n",
    "        torch.nn.init.normal_(self.wRect)\n",
    "        self.wRect=-self.wRect\n",
    "        self.wRect.requires_grad=True\n",
    "        self.dropprob=dropprob\n",
    "        self.sigmaNeu=sigmaNeu\n",
    "        self.wHidden=torch.randn(2*nummotif,32).to(device)\n",
    "        self.wHiddenBias=torch.randn(32).to(device)\n",
    "        if neuType=='nohidden':\n",
    "            \n",
    "            if poolType=='maxavg':\n",
    "                self.wNeu=torch.randn(2*nummotif,1).to(device)\n",
    "            else:\n",
    "                self.wNeu=torch.randn(nummotif,1).to(device)\n",
    "            self.wNeuBias=torch.randn(1).to(device)\n",
    "            torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
    "            torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
    "\n",
    "        else:\n",
    "            if poolType=='maxavg':\n",
    "                self.wHidden=torch.randn(2*nummotif,32).to(device)\n",
    "            else:\n",
    "                \n",
    "                self.wHidden=torch.randn(nummotif,32).to(device)\n",
    "            self.wNeu=torch.randn(32,1).to(device)\n",
    "            self.wNeuBias=torch.randn(1).to(device)\n",
    "            self.wHiddenBias=torch.randn(32).to(device)\n",
    "            torch.nn.init.normal_(self.wNeu,mean=0,std=self.sigmaNeu)\n",
    "            torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.sigmaNeu)\n",
    "            torch.nn.init.normal_(self.wHidden,mean=0,std=0.3)\n",
    "            torch.nn.init.normal_(self.wHiddenBias,mean=0,std=0.3)\n",
    "            \n",
    "  \n",
    "            self.wHidden.requires_grad=True\n",
    "            self.wHiddenBias.requires_grad=True\n",
    "            #wHiddenBias=tf.Variable(tf.truncated_normal([32,1],mean=0,stddev=sigmaNeu)) #hidden bias for everything\n",
    "\n",
    "        self.wNeu.requires_grad=True\n",
    "        self.wNeuBias.requires_grad=True\n",
    "        \n",
    "\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.beta3=beta3\n",
    "        \n",
    "\n",
    "    \n",
    "    def divide_two_tensors(self,x):\n",
    "        l=torch.unbind(x)\n",
    "\n",
    "        list1=[l[2*i] for i in range(int(x.shape[0]/2))]\n",
    "        list2=[l[2*i+1] for i in range(int(x.shape[0]/2))]\n",
    "        x1=torch.stack(list1,0)\n",
    "        x2=torch.stack(list2,0)\n",
    "        return x1,x2\n",
    "    def forward_pass(self,x,mask=None,use_mask=False):\n",
    "        \n",
    "        conv=F.conv1d(x, self.wConv, bias=self.wRect, stride=1, padding=0)\n",
    "        rect=conv.clamp(min=0)\n",
    "        maxPool, _ = torch.max(rect, dim=2)\n",
    "        if self.poolType=='maxavg':\n",
    "            avgPool= torch.mean(rect, dim=2)                          \n",
    "            pool=torch.cat((maxPool, avgPool), 1)\n",
    "        else:\n",
    "            pool=maxPool\n",
    "        if(self.neuType=='nohidden'):\n",
    "            if self.mode=='training': \n",
    "                if  not use_mask:\n",
    "                    mask=bernoulli.rvs(self.dropprob, size=len(pool[0]))\n",
    "                    mask=torch.from_numpy(mask).float().to(device)\n",
    "                pooldrop=pool*mask\n",
    "                out=pooldrop @ self.wNeu\n",
    "                out.add_(self.wNeuBias)\n",
    "            else:\n",
    "                out=self.dropprob*(pool @ self.wNeu)\n",
    "                out.add_(self.wNeuBias)       \n",
    "        else:\n",
    "            hid=pool @ self.wHidden\n",
    "            hid.add_(self.wHiddenBias)\n",
    "            hid=hid.clamp(min=0)\n",
    "            if self.mode=='training': \n",
    "                if  not use_mask:\n",
    "                    mask=bernoulli.rvs(self.dropprob, size=len(hid[0]))\n",
    "                    mask=torch.from_numpy(mask).float().to(device)\n",
    "                hiddrop=hid*mask\n",
    "                out=self.dropprob*(hid @ self.wNeu)\n",
    "                out.add_(self.wNeuBias)\n",
    "            else:\n",
    "                out=self.dropprob*(hid @ self.wNeu)\n",
    "                out.add_(self.wNeuBias) \n",
    "        return out,mask\n",
    "       \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if not  self.reverse_complemet_mode:\n",
    "            out,_=self.forward_pass(x)\n",
    "        else:\n",
    "            \n",
    "            x1,x2=self.divide_two_tensors(x)\n",
    "            out1,mask=self.forward_pass(x1)\n",
    "            out2,_=self.forward_pass(x2,mask,True)\n",
    "            out=torch.max(out1, out2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for model  0  =  0.9190644548476828\n",
      "AUC for model  1  =  0.9139054888391344\n",
      "AUC for model  2  =  0.9134848632878756\n",
      "AUC for model  3  =  0.9212003392360404\n",
      "AUC for model  4  =  0.9223371192075447\n",
      "AUC for model  5  =  0.9161103738381164\n"
     ]
    }
   ],
   "source": [
    "AUC_best = 0\n",
    "learning_steps_list=[4000,8000,12000,16000,20000]\n",
    "\n",
    "best_hyperparameters = torch.load('./Hyperparameters/'+name+'.pth')\n",
    "\n",
    "best_poolType=best_hyperparameters['best_poolType']\n",
    "best_neuType=best_hyperparameters['best_neuType']\n",
    "best_learning_steps=best_hyperparameters['best_learning_steps']\n",
    "best_lr=best_hyperparameters['best_LearningRate']\n",
    "best_droprate=best_hyperparameters['best_dropprob']\n",
    "best_momentum=best_hyperparameters['best_LearningMomentum']\n",
    "best_sigmaConv=best_hyperparameters['best_sigmaConv']\n",
    "best_sigmaNeu=best_hyperparameters['best_sigmaNeu']\n",
    "best_beta1=best_hyperparameters['best_beta1']\n",
    "best_beta2=best_hyperparameters['best_beta2']\n",
    "best_beta3=best_hyperparameters['best_beta3']\n",
    "\n",
    "for number_models in range(6):\n",
    "    model = ConvNet_test(16, 24, best_poolType, best_neuType, 'training', best_lr, best_momentum, best_sigmaConv, best_droprate, best_sigmaNeu, best_beta1, best_beta2, best_beta3, best_learning_steps, False).to(device)\n",
    "\n",
    "    if model.neuType == 'nohidden':\n",
    "        optimizer = torch.optim.SGD([model.wConv, model.wRect, model.wNeu, model.wNeuBias], lr = model.learning_rate, momentum= model.momentum_rate, nesterov=True)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD([model.wConv, model.wRect, model.wNeu, model.wNeuBias, model.wHidden, model.wHiddenBias], lr = model.learning_rate, momentum=model.momentum_rate, nesterov=True)\n",
    "    \n",
    "    train_loader = alldataset_loader\n",
    "    valid_loader = alldataset_loader\n",
    "    learning_steps = 0\n",
    "\n",
    "    while learning_steps <= best_learning_steps:\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            if reverse_mode:\n",
    "                target_2 = torch.randn(int(target.shape[0]/2), 1)\n",
    "                for i in range(target_2.shape[0]):\n",
    "                    target_2[i] = target[2*i]\n",
    "                target = target_2.to(device)\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            if model.neuType == 'nohidden':\n",
    "                loss = F.binary_cross_entropy(torch.sigmoid(output), target) + model.beta1*model.wConv.norm() + model.beta3*model.wNeu.norm()\n",
    "            else: \n",
    "                loss = F.binary_cross_entropy(torch.sigmoid(output), target) + model.beta1*model.wConv.norm() + model.beta2*model.wHidden.norm() + model.beta3*model.wNeu.norm()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            learning_steps += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.mode = 'test'\n",
    "        auc = []\n",
    "        for i, (data, target) in enumerate(valid_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            if reverse_mode:\n",
    "                target_2 = torch.randn(int(target.shape[0]/2), 1)\n",
    "                for i in range(target_2.shape[0]):\n",
    "                    target_2[i] = target[2*i]\n",
    "                target = target_2.to(device)\n",
    "            \n",
    "            # Forward Pass\n",
    "            output = model(data)\n",
    "            pred_sig = torch.sigmoid(output)\n",
    "            pred = pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "\n",
    "            auc.append(metrics.roc_auc_score(labels, pred))\n",
    "        \n",
    "        AUC_training = np.mean(auc)\n",
    "        print('AUC for model ', number_models, ' = ', AUC_training)\n",
    "        if AUC_training > AUC_best:\n",
    "            state = {'conv': model.wConv,'rect':model.wRect,'wHidden':model.wHidden,'wHiddenBias':model.wHiddenBias,'wNeu':model.wNeu,'wNeuBias':model.wNeuBias}\n",
    "            torch.save(state, './Models/'+name+'.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9161103738381164\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('./Models'+name+'.pth')\n",
    "model = ConvNet_test(16, 24, best_poolType, best_neuType, 'test', best_lr, best_momentum, best_sigmaConv, best_droprate, best_sigmaNeu, best_beta1, best_beta2, best_beta3, best_learning_steps, reverse_mode).to(device)\n",
    "model.wConv=checkpoint['conv']\n",
    "model.wRect=checkpoint['rect']\n",
    "model.wHidden=checkpoint['wHidden']\n",
    "model.wHiddenBias=checkpoint['wHiddenBias']\n",
    "model.wNeu=checkpoint['wNeu']\n",
    "model.wNeuBias=checkpoint['wNeuBias']\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.mode = 'test'\n",
    "    auc = []\n",
    "\n",
    "    for i, (data, target) in enumerate(valid_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        if reverse_mode:\n",
    "            target_2 = torch.randn(int(target.shape[0]/2), 1)\n",
    "            for i in range(target_2.shape[0]):\n",
    "                target_2[i] = target[2*i]\n",
    "            target = target_2.to(device)\n",
    "        \n",
    "        # Forward Pass\n",
    "        output = model(data)\n",
    "        pred_sig = torch.sigmoid(output)\n",
    "        pred = pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
    "        labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "\n",
    "        auc.append(metrics.roc_auc_score(labels, pred))\n",
    "\n",
    "    AUC_training = np.mean(auc)\n",
    "    print(AUC_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chip_test():\n",
    "    def __init__(self,filename,motiflen=24,reverse_complemet_mode=reverse_mode):\n",
    "        self.file = filename\n",
    "        self.motiflen = motiflen\n",
    "        self.reverse_complemet_mode=reverse_complemet_mode\n",
    "            \n",
    "    def openFile(self):\n",
    "        test_dataset=[]\n",
    "        with gzip.open(self.file, 'rt') as data:\n",
    "            next(data)\n",
    "            reader = csv.reader(data,delimiter='\\t')\n",
    "            if not self.reverse_complemet_mode:\n",
    "              for row in reader:\n",
    "                      test_dataset.append([seq2pad(row[2],self.motiflen),[int(row[3])]])\n",
    "            else:\n",
    "              for row in reader:\n",
    "                      test_dataset.append([seq2pad(row[2],self.motiflen),[int(row[3])]])\n",
    "                      test_dataset.append([seq2pad(reverse_complement(row[2]),self.motiflen),[int(row[3])]])\n",
    "            \n",
    "        return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on test data =  0.904928\n"
     ]
    }
   ],
   "source": [
    "chipseq_test=Chip_test(dataset_names[1][2])\n",
    "test_data=chipseq_test.openFile()\n",
    "test_dataset=chipseq_dataset(test_data)\n",
    "batchSize=test_dataset.__len__()\n",
    "test_loader = DataLoader(dataset=test_dataset,batch_size=batchSize,shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "      model.mode='test'\n",
    "      auc=[]\n",
    "     \n",
    "      for i, (data, target) in enumerate(test_loader):\n",
    "          data = data.to(device)\n",
    "          target = target.to(device)\n",
    "          if reverse_mode:\n",
    "              target_2=torch.randn(int(target.shape[0]/2),1)\n",
    "              for i in range(target_2.shape[0]):\n",
    "                target_2[i]=target[2*i]\n",
    "              target=target_2.to(device)\n",
    "          # Forward pass\n",
    "          output = model(data)\n",
    "          pred_sig=torch.sigmoid(output)\n",
    "          pred=pred_sig.cpu().detach().numpy().reshape(output.shape[0])\n",
    "          labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "          \n",
    "          auc.append(metrics.roc_auc_score(labels, pred))\n",
    "  #                         \n",
    "      AUC_training=np.mean(auc)\n",
    "      print('AUC on test data = ',AUC_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
